{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from utils import BOS_TOKEN, EOS_TOKEN\n",
    "from utils import load_reuters, save_pretrained, get_loader, init_weights\n",
    "\n",
    "import nltk\n",
    "from collections import defaultdict\n",
    "torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab :             #We map the tokens into indexs.\n",
    "    def __init__(self,tokens= None):\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "        \n",
    "        if tokens is not None :\n",
    "            if \"<unk>\" not in tokens :\n",
    "                tokens = tokens + [\"<unk>\"]\n",
    "            for token in tokens :\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx['<unk>']\n",
    "            \n",
    "    @classmethod\n",
    "    def build(cls,text,min_freq=1,reserved_tokens = None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in text :\n",
    "            for token in sentence :\n",
    "                token_freqs[token] += 1\n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token,freq in token_freqs.items() \\\n",
    "                        if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "\n",
    "\n",
    "def save_vocab(vocab, path):\n",
    "    with open(path, 'w') as writer:\n",
    "        writer.write(\"\\n\".join(vocab.idx_to_token))\n",
    "\n",
    "\n",
    "def read_vocab(path):\n",
    "    with open(path, 'r') as f:\n",
    "        tokens = f.read().split('\\n')\n",
    "    return Vocab(tokens)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramDataset(Dataset):            #Datasets\n",
    "    def __init__(self,corpus,vocab,context_size = 2) :\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        \n",
    "        for sentence in tqdm(corpus, desc = \"Dataset Construction\") : \n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            if len(sentence) < context_size :\n",
    "                continue\n",
    "            for i in range(context_size, len(sentence)) :\n",
    "                context = sentence[i-context_size : i]\n",
    "                target = sentence[i]\n",
    "                self.data.append((context,target))\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return len(self.data)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self,i) :\n",
    "        return self.data[i]\n",
    "    \n",
    "    \n",
    "    def collate_fn(self, examples) :\n",
    "        inputs = torch.tensor([ex[0] for ex in examples], dtype = torch.long)\n",
    "        targets = torch.tensor([ex[1] for ex in examples], dtype = torch.long)\n",
    "        return (inputs,targets)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "class FeedForwardNNLM(nn.Module) :\n",
    "    def __init__(self,vocab_size, embedding_dim, context_size, hidden_dim) :\n",
    "        super(FeedForwardNNLM,self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.activate = F.relu\n",
    "        init_weights(self)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0],-1))\n",
    "        hidden = self.activate(self.linear1(embeds))\n",
    "        output = self.linear2(hidden)\n",
    "        log_probs = F.log_softmax(output,dim=1)\n",
    "        return log_probs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8686515f59174492a8e5a54f1b90c4a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dataset Construction:   0%|          | 0/54711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ada806e2e551466dbebe030ca869a534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 7999.64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79946c408a3340d5ae39415fc724db5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 6616.47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc16b5bf8656400190b119c56f2bfe10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 6003.27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d985eae5c947d797ab0a73e2c3c9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 5565.19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9ece7d7c80473ba7c8a18681cd4d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 5270.40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096e9ff16d5142bd8be34c29ea083980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 5076.78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e83a67dad941ac8714261e67ea513b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4935.61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7139f9f858441da717e2cea8ae78ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4821.23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dd5eab8c2ae40359018dc43ab4aa24d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4724.24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e60db035edb4c0f8226823e0bd44ea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/1521 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss : 4639.32\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "embedding_dim = 64\n",
    "context_size = 5\n",
    "hidden_dim = 256\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "\n",
    "corpus,vocab = load_reuters()\n",
    "dataset = NGramDataset(corpus,vocab,context_size)\n",
    "data_loader = get_loader(dataset, batch_size)\n",
    "\n",
    "nll_loss = nn.NLLLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "model = FeedForwardNNLM(len(vocab), embedding_dim, context_size ,hidden_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr =0.001)\n",
    "\n",
    "\n",
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch) :\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc = f\"Training Epoch {epoch}\"):\n",
    "        inputs,targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss : {total_loss:.2f}\")\n",
    "    total_losses.append(total_loss)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_dim = 64\n",
    "context_size = 2\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "Loss : 9169.07\n",
    "Training Epoch 1: 100%\n",
    "1681/1681 [00:14<00:00, 108.50it/s]\n",
    "Loss : 7822.51\n",
    "Training Epoch 2: 100%\n",
    "1681/1681 [00:14<00:00, 115.62it/s]\n",
    "Loss : 7340.44\n",
    "Training Epoch 3: 100%\n",
    "1681/1681 [00:14<00:00, 116.88it/s]\n",
    "Loss : 7029.79\n",
    "Training Epoch 4: 100%\n",
    "1681/1681 [00:14<00:00, 116.44it/s]\n",
    "Loss : 6810.32\n",
    "Training Epoch 5: 100%\n",
    "1681/1681 [00:14<00:00, 116.74it/s]\n",
    "Loss : 6649.40\n",
    "Training Epoch 6: 100%\n",
    "1681/1681 [00:14<00:00, 113.97it/s]\n",
    "Loss : 6529.11\n",
    "Training Epoch 7: 100%\n",
    "1681/1681 [00:14<00:00, 118.39it/s]\n",
    "Loss : 6434.87\n",
    "Training Epoch 8: 100%\n",
    "1681/1681 [00:14<00:00, 108.42it/s]\n",
    "Loss : 6356.45\n",
    "Training Epoch 9: 100%\n",
    "1681/1681 [00:14<00:00, 116.71it/s]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_dim = 64\n",
    "context_size = 5\n",
    "hidden_dim = 128\n",
    "batch_size = 1024\n",
    "num_epoch = 10\n",
    "\n",
    "Dataset Construction: 100%\n",
    "54711/54711 [00:01<00:00, 43560.28it/s]\n",
    "Training Epoch 0: 100%\n",
    "1521/1521 [00:14<00:00, 111.04it/s]\n",
    "Loss : 8213.62\n",
    "Training Epoch 1: 100%\n",
    "1521/1521 [00:13<00:00, 110.07it/s]\n",
    "Loss : 6897.06\n",
    "Training Epoch 2: 100%\n",
    "1521/1521 [00:13<00:00, 110.72it/s]\n",
    "Loss : 6397.75\n",
    "Training Epoch 3: 100%\n",
    "1521/1521 [00:13<00:00, 110.83it/s]\n",
    "Loss : 6050.45\n",
    "Training Epoch 4: 100%\n",
    "1521/1521 [00:13<00:00, 110.56it/s]\n",
    "Loss : 5788.94\n",
    "Training Epoch 5: 100%\n",
    "1521/1521 [00:13<00:00, 109.98it/s]\n",
    "Loss : 5592.05\n",
    "Training Epoch 6: 100%\n",
    "1521/1521 [00:13<00:00, 103.47it/s]\n",
    "Loss : 5448.94\n",
    "Training Epoch 7: 100%\n",
    "1521/1521 [00:13<00:00, 112.62it/s]\n",
    "Loss : 5340.46\n",
    "Training Epoch 8: 100%\n",
    "1521/1521 [00:13<00:00, 112.47it/s]\n",
    "Loss : 5252.75\n",
    "Training Epoch 9: 100%\n",
    "1521/1521 [00:13<00:00, 110.87it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained embeddings saved to: ffnnlm.vec\n"
     ]
    }
   ],
   "source": [
    "save_pretrained(vocab, model.embeddings.weight.data, \"ffnnlm.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73e43853552d6fc356b2c05e3ac4c7e6c4dba3aed2cbc26a744008cb0f5a4666"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
